{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a806bb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ankit's\n",
      "[nltk_data]     PREDATOR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ankit's\n",
      "[nltk_data]     PREDATOR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Ankit's\n",
      "[nltk_data]     PREDATOR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e425d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.google.com/spreadsheets/d/1O0OA2O9sK9mwmCecdHqol-cI-pbLnyJo/edit?usp=sharing&ouid=117752925753966660868&rtpof=true&sd=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d51facfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_id = '1O0OA2O9sK9mwmCecdHqol-cI-pbLnyJo'\n",
    "df = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f29aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' column to datetime type\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "# Convert text columns to string type\n",
    "df['Company'] = df['Company'].astype(str)\n",
    "df['Location'] = df['Location'].astype(str)\n",
    "df['Job Title'] = df['Job Title'].astype(str)\n",
    "df['Job Description'] = df['Job Description'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c59a4147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Stopwords from job description\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply stopwords removal\n",
    "df['Job Description'] = df['Job Description'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f9f9e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ankit's PREDATOR\\AppData\\Local\\Temp\\ipykernel_15788\\1588144625.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Job Description'] = df['Job Description'].str.replace('[^\\w\\s]', '')  # Remove punctuation\n"
     ]
    }
   ],
   "source": [
    "#Basic Preprocessing\n",
    "df['Job Description'] = df['Job Description'].str.lower()  # Convert to lower case\n",
    "df['Job Description'] = df['Job Description'].str.replace('[^\\w\\s]', '')  # Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a509b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Skillsets\n",
    "tools = {\n",
    "    'Airflow', 'Alteryx', 'Apache Cassandra', 'Apache Kafka', 'AWS Redshift', \n",
    "    'AWS S3', 'Azure Data Factory', 'Azure Synapse', 'BigQuery', 'Bokeh', \n",
    "    'Cognos', 'DB2', 'D3.js', 'DataRobot', 'Domo', 'Elasticsearch', 'Excel', \n",
    "    'GCP (Google Cloud Platform)', 'Git', 'GitHub', 'Google Analytics', \n",
    "    'Google Sheets', 'Greenplum', 'Hadoop', 'Hive', 'JIRA', 'Jupyter Notebook', \n",
    "    'KNIME', 'Looker', 'MATLAB', 'Mode Analytics', 'MongoDB', 'MySQL', 'Neo4j', \n",
    "    'NoSQL databases', 'Oracle Database', 'Pandas', 'PostgreSQL', 'Power BI', \n",
    "    'Python', 'QlikView', 'R', 'RapidMiner', 'SAP BusinessObjects', 'SAS', \n",
    "    'Seaborn', 'Snowflake', 'SPSS', 'SQL', 'Splunk', 'SSIS (SQL Server Integration Services)', \n",
    "    'SSRS (SQL Server Reporting Services)', 'Stata', 'Tableau', 'Talend', 'T-SQL', \n",
    "    'Trello', 'VBA (Visual Basic for Applications)', 'Visual Studio Code', \n",
    "    'Watson Analytics', 'Xplenty', 'Yellowfin BI', 'Zoho Analytics'\n",
    "}\n",
    "\n",
    "technical_skills = {\n",
    "    'A/B Testing', 'Algorithm Development', 'API Integration', 'Automated Reporting', \n",
    "    'Big Data Analysis', 'Business Intelligence (BI) Reporting', 'Cluster Analysis', \n",
    "    'Coding (Python, R, SQL, etc.)', 'Correlation and Regression Analysis', \n",
    "    'Critical Thinking', 'Cross-Validation', 'Data Architecture', 'Data Blending', \n",
    "    'Data Cleaning', 'Data Collection', 'Data Engineering', 'Data Extraction', \n",
    "    'Data Governance', 'Data Integration', 'Data Manipulation', 'Data Mining', \n",
    "    'Data Modeling', 'Data Quality Assessment', 'Data Scraping', 'Data Transformation', \n",
    "    'Data Validation', 'Database Management', 'Database Querying', 'Decision Trees', \n",
    "    'Descriptive Statistics', 'Dimensional Modeling', 'ETL (Extract, Transform, Load) Processes', \n",
    "    'Experimental Design', 'Exploratory Data Analysis (EDA)', 'Feature Engineering', \n",
    "    'Forecasting', 'Hypothesis Testing', 'Indexing and Partitioning', 'KPI Development', \n",
    "    'Linear and Nonlinear Modeling', 'Machine Learning', 'Mathematical Modeling', \n",
    "    'Natural Language Processing (NLP)', 'Neural Networks', 'Optimization Techniques', \n",
    "    'Pattern Recognition', 'Predictive Analytics', 'Prescriptive Analytics', \n",
    "    'Probability Theory', 'Programming Logic', 'Query Optimization', \n",
    "    'Relational Database Management', 'Report Automation', 'Risk Analysis', \n",
    "    'Segmentation', 'Sentiment Analysis', 'Statistical Analysis', 'Statistical Programming', \n",
    "    'Survey Analysis', 'Survival Analysis', 'Time Series Analysis', 'Trend Analysis', \n",
    "    'Version Control (Git)', 'Visualization Design (Charts, Dashboards, etc.)', \n",
    "    'Web Analytics', 'Workflow Automation'\n",
    "}\n",
    "\n",
    "soft_skills = {\n",
    "    'Active Listening', 'Adaptability', 'Analytical Thinking', 'Assertiveness', \n",
    "    'Attention to Detail', 'Change Management', 'Client Management', 'Collaboration', \n",
    "    'Communication', 'Conflict Resolution', 'Creativity', 'Critical Thinking', \n",
    "    'Cultural Awareness', 'Curiosity', 'Decision-Making', 'Emotional Intelligence', \n",
    "    'Empathy', 'Facilitation', 'Flexibility', 'Influencing', 'Innovation', \n",
    "    'Interpersonal Skills', 'Leadership', 'Listening', 'Mentoring', 'Negotiation', \n",
    "    'Networking', 'Open-Mindedness', 'Organizational Skills', 'Patience', 'Persuasion', \n",
    "    'Proactivity', 'Problem-Solving', 'Problem Sensitivity', 'Project Management', \n",
    "    'Relationship Building', 'Relationship Management', 'Resilience', 'Resourcefulness', \n",
    "    'Sales Skills', 'Self-Motivation', 'Stakeholder Management', 'Strategic Thinking', \n",
    "    'Stress Management', 'Team Building', 'Teamwork', 'Time Management', 'Tact', \n",
    "    'Trustworthiness', 'Verbal Communication', 'Work Ethic', 'Written Communication'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62e92dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def extract_skills(description, skills_set):\n",
    "    skill_counter = Counter()\n",
    "    description = description.lower()  # Convert to lowercase for consistent matching\n",
    "    # Tokenize the description using regular expressions to handle punctuation\n",
    "    words = re.findall(r'\\b\\w+\\b', description)\n",
    "    \n",
    "    for skill in skills_set:\n",
    "        # Normalize skill for matching\n",
    "        normalized_skill = skill.lower()\n",
    "        # Check if the skill is in the list of words from the description\n",
    "        if normalized_skill in ' '.join(words):\n",
    "            skill_counter[skill] += 1\n",
    "    return skill_counter\n",
    "\n",
    "# Apply function to the dataset\n",
    "df['tools'] = df['Job Description'].apply(lambda x: extract_skills(x, tools))\n",
    "df['technical_skills'] = df['Job Description'].apply(lambda x: extract_skills(x, technical_skills))\n",
    "df['soft_skills'] = df['Job Description'].apply(lambda x: extract_skills(x, soft_skills))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9e28a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d700f77c3e4d90b3039887f48733e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Job Title:', layout=Layout(width='210px'), options=('All', 'Data Analyst â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671ca0d1653f4839a89a1b2d15d1c9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Set a Seaborn style and color palette\n",
    "palette = sns.color_palette(\"viridis\", n_colors=20)\n",
    "\n",
    "# Date Picker Widgets\n",
    "start_date_picker = widgets.DatePicker(\n",
    "    description='Start Date:',\n",
    "    value=pd.to_datetime(df['Date']).min().date(),  # Ensure proper datetime handling\n",
    "    layout=widgets.Layout(width='210px')\n",
    ")\n",
    "\n",
    "end_date_picker = widgets.DatePicker(\n",
    "    description='End Date:',\n",
    "    value=pd.to_datetime(df['Date']).max().date(),  # Ensure proper datetime handling\n",
    "    layout=widgets.Layout(width='210px')\n",
    ")\n",
    "\n",
    "# Dropdown for Job Titles\n",
    "job_titles = df['Job Title'].unique()\n",
    "job_title_dropdown = widgets.Dropdown(\n",
    "    options=['All'] + list(job_titles),\n",
    "    value='All',\n",
    "    description='Job Title:',\n",
    "    layout=widgets.Layout(width='210px')\n",
    ")\n",
    "\n",
    "# Dropdown for Locations\n",
    "locations = df['Location'].unique()\n",
    "location_dropdown = widgets.Dropdown(\n",
    "    options=['All'] + list(locations),\n",
    "    value='All',\n",
    "    description='Location:',\n",
    "    layout=widgets.Layout(width='210px')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Function to filter data based on widgets\n",
    "def filter_data(job_title, location, start_date, end_date):\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    # Convert 'Date' column to datetime if it's not already\n",
    "    filtered_df['Date'] = pd.to_datetime(filtered_df['Date'], errors='coerce')\n",
    "    \n",
    "    if job_title != 'All':\n",
    "        filtered_df = filtered_df[filtered_df['Job Title'] == job_title]\n",
    "    \n",
    "    if location != 'All':\n",
    "        filtered_df = filtered_df[filtered_df['Location'] == location]\n",
    "    \n",
    "    if start_date and end_date:\n",
    "        # Filter by date range\n",
    "        filtered_df = filtered_df[\n",
    "            (filtered_df['Date'] >= pd.to_datetime(start_date)) & \n",
    "            (filtered_df['Date'] <= pd.to_datetime(end_date))\n",
    "        ]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Function to plot skills with Seaborn and annotations\n",
    "def plot_skills(filtered_df, skill_type, ax):\n",
    "    skill_counter = Counter()\n",
    "    for description in filtered_df['Job Description']:\n",
    "        skill_counter.update(extract_skills(description, globals()[skill_type]))\n",
    "    \n",
    "    top_skills = skill_counter.most_common(20)\n",
    "    \n",
    "    if not top_skills:\n",
    "        ax.text(0.5, 0.5, 'No Data Available', ha='center', va='center')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        return\n",
    "    \n",
    "    skills, counts = zip(*top_skills)\n",
    "    \n",
    "    sns.barplot(x=list(counts), y=list(skills), palette=palette, ax=ax)\n",
    "    ax.set_xlabel('Number of Jobs')\n",
    "    ax.set_ylabel('Skills')\n",
    "    ax.set_title(f'Top {skill_type.replace(\"_\", \" \").capitalize()}')\n",
    "    \n",
    "    # Add annotations on bars\n",
    "    for i, count in enumerate(counts):\n",
    "        ax.text(count, i, f'{count}', va='center', ha='left', fontsize=12)\n",
    "\n",
    "def update_display(*args):\n",
    "    with output_box:\n",
    "        output_box.clear_output()  # Clear previous output\n",
    "        filtered_df = filter_data(\n",
    "            job_title_dropdown.value, \n",
    "            location_dropdown.value, \n",
    "            start_date_picker.value, \n",
    "            end_date_picker.value\n",
    "        )\n",
    "        \n",
    "        fig, axs = plt.subplots(1, 3, figsize=(20, 8))\n",
    "        plot_skills(filtered_df, 'tools', axs[0])\n",
    "        plot_skills(filtered_df, 'technical_skills', axs[1])\n",
    "        plot_skills(filtered_df, 'soft_skills', axs[2])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Link the function to widget updates\n",
    "job_title_dropdown.observe(update_display, 'value')\n",
    "location_dropdown.observe(update_display, 'value')\n",
    "start_date_picker.observe(update_display, 'value')\n",
    "end_date_picker.observe(update_display, 'value')\n",
    "\n",
    "# Layout for filters and plots\n",
    "filters_box = widgets.HBox([job_title_dropdown, location_dropdown, start_date_picker, end_date_picker],\n",
    "                           layout=widgets.Layout(width='auto', justify_content='space-between'))\n",
    "\n",
    "output_box = widgets.Output()\n",
    "\n",
    "# Display widgets and output together\n",
    "display(filters_box, output_box)\n",
    "\n",
    "with output_box:\n",
    "    update_display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1c0637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
